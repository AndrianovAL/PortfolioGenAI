{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c1b596",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4cb8e",
   "metadata": {},
   "source": [
    "In this example, I am using LangChain to build a simple LLM-powered assistant. I will focus on using:\n",
    "1. Google Gemini\n",
    "2. OpenAI gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdf314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   langchain-core==0.3.33 \\\n",
    "#   langchain-openai==0.3.3 \\\n",
    "#   langchain-community==0.3.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ea039",
   "metadata": {},
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576be773",
   "metadata": {},
   "source": [
    "First need to initialize an LLM. I'll use _Google Gemini_ or _OpenAI 'gpt-4.1-nano'_ model. You can get an API key from [OpenAI](https://platform.openai.com/settings/organization/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a77982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_Provider = \"GOOGLE\"\n",
    "# LLM_Provider = \"OPENAI\"\n",
    "\n",
    "if AI_Provider == \"GOOGLE\":\n",
    "    llm_model = \"gemini-2.0-flash\"\n",
    "elif AI_Provider == \"OPENAI\":\n",
    "    llm_model = \"gpt-4.1-nano\"  # less expensive than gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f878cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GOOGLE_API_KEY loaded successfully (not printing it for security).\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "load_dotenv()  # Try to load local .env file (for local dev); silently skip if not found (for CI)\n",
    "os.environ[f\"{AI_Provider}_API_KEY\"] = os.getenv(f\"{AI_Provider}_API_KEY\") or getpass(f\"Enter {AI_Provider} API Key: \")  # Get API key from environment or user input\n",
    "if os.getenv(f\"{AI_Provider}_API_KEY\") is None:\n",
    "    raise ValueError(f\"❌ {AI_Provider}_API_KEY not found. Make sure it's in your .env file or set as a GitHub Action secret.\")\n",
    "else:\n",
    "    print(f\"✅ {AI_Provider}_API_KEY loaded successfully (not printing it for security).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cc9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AI_Provider == \"GOOGLE\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI as ChatLLM\n",
    "elif AI_Provider == \"OPENAI\":\n",
    "    from langchain_openai import ChatOpenAI as ChatLLM\n",
    "\n",
    "llm = ChatLLM(temperature=0.0, model=llm_model)  # For normal accurate responses\n",
    "# print(\"✅\", llm.invoke(\"What LLM version are you?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec980a",
   "metadata": {},
   "source": [
    "# Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48382bba",
   "metadata": {},
   "source": [
    "I'll take a `new_report.txt`, `metrics.csv`, and `training_examples.csv` and use LangChain to:\n",
    "1. Score the new report\n",
    "2. Extract from the new_report 3 pieces of supporting evidence for this Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6108c9b",
   "metadata": {},
   "source": [
    "# Create VectorDB from TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b5cc5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250  # FIXME: calibrate\n",
    "chunk_overlap = 50  # FIXME: calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92edff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report loaded successfully!\n",
      "Report length: 2591 characters\n",
      "First 200 characters of the report:\n",
      "Balanced Power of IBM\n",
      "\n",
      "IBM’s latest sustainability report opens with a bold statement: “IBM is committed to achieving net-zero GHG Scope 1 emissions by 2050 through a comprehensive, three-phase decarb...\n"
     ]
    }
   ],
   "source": [
    "# Read the IBM report file into a string\n",
    "with open('data/new_report_IBM.txt', 'r', encoding='utf-8') as file:\n",
    "    report = file.read()\n",
    "print(\"Report loaded successfully!\")\n",
    "print(f\"Report length: {len(report)} characters\")\n",
    "print(\"First 200 characters of the report:\")\n",
    "print(report[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faebc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chunks is 16\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # to split YouTube transcript into chunks\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=report)]  # wrap the report string in a Document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)  # initilize the text splitter\n",
    "docs = text_splitter.split_documents(documents)  # split report into overlapping chunks\n",
    "print(f'The number of chunks is {len(docs)}')\n",
    "# # test\n",
    "# print(f'Test: docs[0].page_content:\\n{docs[0].page_content}')\n",
    "# for i, doc in enumerate(docs):\n",
    "#     print(f'Test: docs[{i}].page_content:\\n{doc.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55f2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_DT\\Prog\\_MyCode\\ed_AI_2025\\PortfolioGenAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings  # Alternative:  from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    # Try:          from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# define how to map a string (can be a sentence or a paragraph) to a vector\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # This model is small (~80MB), fast on CPU, good for English # Alternative: ultra-fast memory-light (~45MB): model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\" # Alternative: embeddings = OpenAIEmbeddings()\n",
    "# # Test: one embed string\n",
    "# embed_test = embeddings.embed_query(\"What company is the report about?\")\n",
    "# print(f'Test: len(embeddings)={len(embed_test)}, embeddings[:5]={embed_test[:5]}')  # Should return a 384-dim vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3e4c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of vectors in the DB is 16\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS  # Vector Database (indexes); alternatives: Pinecon, Weaviate\n",
    "db = FAISS.from_documents(docs, embeddings)  # create a DB of vector embeddings from the docs\n",
    "print(f'The number of vectors in the DB is {db.index.ntotal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0bbd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectordb_from_report(report_filename: str) -> FAISS:\n",
    "    return db # TODO: Implement this function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a3262",
   "metadata": {},
   "source": [
    "# Simularity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prep_simularity_search_query(metrics, metric_id):\n",
    "def prep_simularity_search_query(metrics, metric_id):\n",
    "\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"{metric_name} metric:\\n{metric_description}\"\"\",\n",
    "        input_variables=[\"metric_name\", \"metric_description\"]\n",
    "    )\n",
    "\n",
    "    query = user_prompt.format(\n",
    "        metric_name = metrics[metrics[\"MetricID\"]==metric_id][\"MetricName\"].values[0],\n",
    "        metric_description = metrics[metrics[\"MetricID\"]==metric_id][\"MetricDescription\"].values[0]).content\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f216d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simularity_search(query, db, chunks_number=4):\n",
    "    docs = db.similarity_search(query, k=chunks_number)  # find chunks_number docs similar to the user's query; FAISS does the similarity search\n",
    "    new_company_report_chunks_summary = \" \".join([doc.page_content for doc in docs])  # combine \"page_content\" fields from each of the found docs\n",
    "    return new_company_report_chunks_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce516c88",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3fecc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics_filename = 'data/metrics.csv'\n",
    "metrics = pd.read_csv(metrics_filename)\n",
    "# print(metrics.iloc[0][\"MetricDescription\"])\n",
    "# metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "78857215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples_filename = 'data/train_examples.csv'\n",
    "train_examples = pd.read_csv(train_examples_filename)\n",
    "# train_examples.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5fb9951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prompt_inputs(new_company, metrics, metric_id, train_examples, new_company_report_chunks_summary):\n",
    "    prompt_inputs = {\n",
    "        'new_company': new_company,\n",
    "        'metric_name': metrics[metrics[\"MetricID\"]==metric_id][\"MetricName\"].values[0],\n",
    "        'metric_description': metrics[metrics[\"MetricID\"]==metric_id][\"MetricDescription\"].values[0]\n",
    "    }\n",
    "\n",
    "    # add inputs from exaples\n",
    "    df = train_examples[train_examples['MetricID']==metric_id].reset_index(drop=True)\n",
    "    assert len(df) == 3, \"Expected exactly 3 example companies for 1 metric\"  # Safety check\n",
    "\n",
    "    prompt_inputs['Company_1'] = df.loc[0, 'Company']\n",
    "    prompt_inputs['Score_1'] = df.loc[0, 'Score']\n",
    "    prompt_inputs['Reasons1_1'] = df.loc[0, 'Reason1']\n",
    "    prompt_inputs['Reasons2_1'] = df.loc[0, 'Reason2']\n",
    "    prompt_inputs['Reasons3_1'] = df.loc[0, 'Reason3']\n",
    "    \n",
    "    prompt_inputs['Company_2'] = df.loc[1, 'Company']\n",
    "    prompt_inputs['Score_2'] = df.loc[1, 'Score']\n",
    "    prompt_inputs['Reasons1_2'] = df.loc[1, 'Reason1']\n",
    "    prompt_inputs['Reasons2_2'] = df.loc[1, 'Reason2']\n",
    "    prompt_inputs['Reasons3_2'] = df.loc[1, 'Reason3']\n",
    "    \n",
    "    prompt_inputs['Company_3'] = df.loc[2, 'Company']\n",
    "    prompt_inputs['Score_3'] = df.loc[2, 'Score']\n",
    "    prompt_inputs['Reasons1_3'] = df.loc[2, 'Reason1']\n",
    "    prompt_inputs['Reasons2_3'] = df.loc[2, 'Reason2']\n",
    "    prompt_inputs['Reasons3_3'] = df.loc[2, 'Reason3']\n",
    "\n",
    "    prompt_inputs['new_company_report_chunks_summary'] = new_company_report_chunks_summary\n",
    "    \n",
    "    return prompt_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2e7da845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template('You are a sustainability consultant tasked to score a company against the provided metric. Score can be: 1, 2, or 3.')\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"Metric name: {metric_name}\n",
    "    Scoring criteria: {metric_description}\n",
    "\n",
    "    # You need to score the company \"{new_company}\" against this metric and provide 3 reasons for the score.\n",
    "        ## The output should be a JSON object with the following fields (no other explanation or text or fields are allowed):\n",
    "        - Company: the name of the company\n",
    "        - Score: the score of the company\n",
    "        - Reasons1: first reason for the score\n",
    "        - Reasons2: second reason for the score\n",
    "        - Reasons3: third reason for the score\n",
    "\n",
    "    # Below are examples of the scoring applied to 3 companies:\n",
    "    Company 1: {Company_1}\n",
    "    Score: {Score_1}\n",
    "    Reason 1: {Reasons1_1}\n",
    "    Reason 2: {Reasons2_1}\n",
    "    Reason 3: {Reasons3_1}\n",
    "\n",
    "    Company 2: {Company_2}\n",
    "    Score: {Score_2}\n",
    "    Reason 1: {Reasons1_2}\n",
    "    Reason 2: {Reasons2_2}\n",
    "    Reason 3: {Reasons3_2}\n",
    "\n",
    "    Company 3: {Company_3}\n",
    "    Score: {Score_3}\n",
    "    Reason 1: {Reasons1_3}\n",
    "    Reason 2: {Reasons2_3}\n",
    "    Reason 3: {Reasons3_3}\n",
    "    \n",
    "    # The report of the company \"{new_company}\" is:\n",
    "    {new_company_report_chunks_summary}\n",
    "    \"\"\",\n",
    "\n",
    "    input_variables=[\"metric_name\", \"metric_description\", \"new_company\",\n",
    "        \"Company_1\", \"Score_1\", \"Reasons1_1\", \"Reasons2_1\", \"Reasons3_1\",\n",
    "        \"Company_2\", \"Score_2\", \"Reasons1_2\", \"Reasons2_2\", \"Reasons3_2\",\n",
    "        \"Company_3\", \"Score_3\", \"Reasons1_3\", \"Reasons2_3\", \"Reasons3_3\", \"new_company_report_chunks_summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ba951",
   "metadata": {},
   "source": [
    "Now we can merge the system and user prompts into a full chat prompt using the `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2d78faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt template 1: create an article title\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad6879",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2693c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d8cbe",
   "metadata": {},
   "source": [
    "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a9c27e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt.format(**prompt_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ac1e",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_company = \"IBM\"  # FIXME\n",
    "for i in range(1):  #FIXME: range(len(metrics)):\n",
    "    metric_id = metrics.iloc[i][\"MetricID\"]\n",
    "    print(f\"MetricID: {metric_id}\")\n",
    "\n",
    "    query = prep_simularity_search_query(metrics, metric_id)\n",
    "    print(query)\n",
    "\n",
    "    new_company_report_chunks_summary = simularity_search(query, db, number_of_chunks=3)\n",
    "    print(f\"new_company_report_chunks_summary: {new_company_report_chunks_summary}\")\n",
    "\n",
    "    prompt_inputs = prep_prompt_inputs(new_company, metrics, metric_id, train_examples)\n",
    "    #TODO: run the LLM to score the new company and extract 3 reasons for the score\n",
    "    #TODO: record the score in the output dataframe\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0c945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n  \"Company\": \"IBM\",\\n  \"Score\": 3,\\n  \"Reasons1\": \"IBM has committed to net zero GHG emissions by 2030, encompassing Scope 1 emissions.\",\\n  \"Reasons2\": \"IBM provides a detailed roadmap with specific targets and initiatives for reducing Scope 1 emissions year by year.\",\\n  \"Reasons3\": \"IBM has already implemented several initiatives and achieved significant reductions in its Scope 1 emissions, demonstrating progress towards its 2030 goal.\"\\n}\\n```', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--9f10a023-373d-436b-bc47-c9bc44f405a4-0', usage_metadata={'input_tokens': 656, 'output_tokens': 114, 'total_tokens': 770, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(prompt.format(**prompt_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9e39e615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Company': 'IBM',\n",
       " 'Score': 3,\n",
       " 'Reasons1': 'IBM has committed to net zero GHG emissions by 2030, encompassing Scope 1 emissions.',\n",
       " 'Reasons2': 'IBM provides a detailed roadmap with specific targets and initiatives for reducing Scope 1 emissions year by year.',\n",
       " 'Reasons3': 'IBM has already implemented several initiatives and achieved significant reductions in its Scope 1 emissions, demonstrating progress towards its 2030 goal.'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Step 1: Extract the JSON part (removes ```json ... ```)\n",
    "raw_output = output.content\n",
    "json_str = re.sub(r\"^```json|```$\", \"\", raw_output.strip()).strip()\n",
    "\n",
    "# Step 2: Convert to Python dict\n",
    "parsed_output = json.loads(json_str)\n",
    "\n",
    "# Check the result\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3fc410e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>MetricID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Reasons1</th>\n",
       "      <th>Reasons2</th>\n",
       "      <th>Reasons3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>IBM has committed to net zero GHG emissions by...</td>\n",
       "      <td>IBM provides a detailed roadmap with specific ...</td>\n",
       "      <td>IBM has already implemented several initiative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company  MetricID  Score                                           Reasons1  \\\n",
       "0     IBM         1      3  IBM has committed to net zero GHG emissions by...   \n",
       "\n",
       "                                            Reasons2  \\\n",
       "0  IBM provides a detailed roadmap with specific ...   \n",
       "\n",
       "                                            Reasons3  \n",
       "0  IBM has already implemented several initiative...  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_i = pd.DataFrame(parsed_output, index=[0])\n",
    "res_i.insert(1, \"MetricID\", metric_id)\n",
    "res_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2936b",
   "metadata": {},
   "source": [
    "Now chain the `prompt` template and the `llm` object defined earlier to create an LLM chain for **prompt formatting > llm generation > get output**.\n",
    "\n",
    "Let's use __LCEL__ to construct the chain: define inputs with `{\"article\": lambda x: x[\"article\"]}` and use the pipe operator (`|`) to feed the output from its left into the input to its right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d434bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain1: inputs:article / output:article_title\n",
    "chain = (\n",
    "    # {\n",
    "    #     \"article\": lambda x: x[\"article\"],\n",
    "    #     \"article_title\": lambda x: x[\"article_title\"]\n",
    "    # }\n",
    "    # | \n",
    "    prompt\n",
    "    | llm\n",
    "    | {\"response\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7615599",
   "metadata": {},
   "source": [
    "This chain creates scores the new company and procides 3 reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "75a772e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"Company\": \"IBM\",\\n  \"Score\": 3,\\n  \"Reasons1\": \"IBM has committed to net zero GHG emissions by 2030, encompassing Scope 1 emissions.\",\\n  \"Reasons2\": \"IBM provides a detailed roadmap with specific initiatives and timelines for achieving its net zero target, including interim goals and progress reporting.\",\\n  \"Reasons3\": \"IBM has already implemented several initiatives and demonstrated progress towards its GHG reduction targets, indicating that the first steps have been taken.\"\\n}\\n```'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = chain.invoke({\n",
    "#     \"article\": article,\n",
    "#     \"article_title\": article_title_msg[\"article_title\"]\n",
    "# })\n",
    "res = chain.invoke(prompt_inputs)\n",
    "res[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad28f18",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> __NEXT__ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5611b3",
   "metadata": {},
   "source": [
    "Since we output _several fields_ we'll specify for the LLM to use __structured outputs__ to make the generated fields aligned with our requirements.\n",
    "or this we create a _pydantic object_ and describe the required output format - this format description is then passed to our model using the `with_structured_output` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f0a4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    paragraph_original: str = Field(description=\"The original paragraph\")\n",
    "    paragraph_edited: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(description=\"Feedback on the original paragraph\")\n",
    "\n",
    "llm_structured = llm_creative.with_structured_output(Paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c24f9d",
   "metadata": {},
   "source": [
    "Let's combine all this into chain3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af74e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain3: inputs:article / output:paragraph\n",
    "chain3 = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt3\n",
    "    | llm_structured\n",
    "    | {\n",
    "        \"paragraph_original\": lambda x: x.paragraph_original,\n",
    "        \"paragraph_edited\": lambda x: x.paragraph_edited,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3273870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph_original': 'I tackled a pressing issue for a large life insurance client: their computational model for projecting financial outcomes and predicting balance sheets over multiple quarters for life insurance products was prohibitively slow and costly. The client’s leadership needed a capability to run ad-hoc what-if scenarios, but the model’s runtime prevented near-real-time risk analytics.',\n",
       " 'paragraph_edited': 'I tackled a pressing issue for a large life insurance client: their computational model for projecting financial outcomes and predicting balance sheets over multiple quarters for life insurance products was prohibitively slow and costly. This inefficiency hindered their ability to respond quickly to market changes and increased the risk of inaccurate financial forecasting. The client’s leadership needed a capability to run ad-hoc what-if scenarios, but the model’s runtime prevented near-real-time risk analytics.',\n",
       " 'feedback': \"The original paragraph is well-written and clearly explains the problem. However, it could be improved by adding a sentence or two that highlight the impact of the slow and costly model on the client's business operations. This would provide additional context and emphasize the importance of solving the problem.\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_and_feedback = chain3.invoke({\"article\": article})\n",
    "paragraph_and_feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
