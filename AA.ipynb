{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c1b596",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4cb8e",
   "metadata": {},
   "source": [
    "In this example, I am using LangChain to build a simple LLM-powered assistant. I will focus on using:\n",
    "1. Google Gemini\n",
    "2. OpenAI gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdf314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   langchain-core==0.3.33 \\\n",
    "#   langchain-openai==0.3.3 \\\n",
    "#   langchain-community==0.3.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ea039",
   "metadata": {},
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576be773",
   "metadata": {},
   "source": [
    "First need to initialize an LLM. I'll use _Google Gemini_ or _OpenAI 'gpt-4.1-nano'_ model. You can get an API key from [OpenAI](https://platform.openai.com/settings/organization/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a77982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_Provider = \"GOOGLE\"\n",
    "# LLM_Provider = \"OPENAI\"\n",
    "\n",
    "if AI_Provider == \"GOOGLE\":\n",
    "    llm_model = \"gemini-2.0-flash\"\n",
    "elif AI_Provider == \"OPENAI\":\n",
    "    llm_model = \"gpt-4.1-nano\"  # less expensive than gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f878cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GOOGLE_API_KEY loaded successfully (not printing it for security).\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "load_dotenv()  # Try to load local .env file (for local dev); silently skip if not found (for CI)\n",
    "os.environ[f\"{AI_Provider}_API_KEY\"] = os.getenv(f\"{AI_Provider}_API_KEY\") or getpass(f\"Enter {AI_Provider} API Key: \")  # Get API key from environment or user input\n",
    "if os.getenv(f\"{AI_Provider}_API_KEY\") is None:\n",
    "    raise ValueError(f\"❌ {AI_Provider}_API_KEY not found. Make sure it's in your .env file or set as a GitHub Action secret.\")\n",
    "else:\n",
    "    print(f\"✅ {AI_Provider}_API_KEY loaded successfully (not printing it for security).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cc9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AI_Provider == \"GOOGLE\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI as ChatLLM\n",
    "elif AI_Provider == \"OPENAI\":\n",
    "    from langchain_openai import ChatOpenAI as ChatLLM\n",
    "\n",
    "llm = ChatLLM(temperature=0.0, model=llm_model)  # For normal accurate responses\n",
    "# print(\"✅\", llm.invoke(\"What LLM version are you?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec980a",
   "metadata": {},
   "source": [
    "# Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48382bba",
   "metadata": {},
   "source": [
    "I'll take a `new_report.txt`, `metrics.csv`, and `training_examples.csv` and use LangChain to:\n",
    "1. Score the new report\n",
    "2. Extract from the new_report 3 pieces of supporting evidence for this Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6108c9b",
   "metadata": {},
   "source": [
    "# Create VectorDB from TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cc5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250  # FIXME: calibrate\n",
    "chunk_overlap = 50  # FIXME: calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92edff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report loaded successfully!\n",
      "Report length: 2591 characters\n",
      "First 200 characters of the report:\n",
      "Balanced Power of IBM\n",
      "\n",
      "IBM’s latest sustainability report opens with a bold statement: “IBM is committed to achieving net-zero GHG Scope 1 emissions by 2050 through a comprehensive, three-phase decarb...\n"
     ]
    }
   ],
   "source": [
    "# Read the IBM report file into a string\n",
    "with open('data/new_report_IBM.txt', 'r', encoding='utf-8') as file:\n",
    "    report = file.read()\n",
    "print(\"Report loaded successfully!\")\n",
    "print(f\"Report length: {len(report)} characters\")\n",
    "print(\"First 200 characters of the report:\")\n",
    "print(report[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faebc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chunks is 16\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # to split YouTube transcript into chunks\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=report)]  # wrap the report string in a Document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)  # initilize the text splitter\n",
    "docs = text_splitter.split_documents(documents)  # split report into overlapping chunks\n",
    "print(f'The number of chunks is {len(docs)}')\n",
    "# # test\n",
    "# print(f'Test: docs[0].page_content:\\n{docs[0].page_content}')\n",
    "# for i, doc in enumerate(docs):\n",
    "#     print(f'Test: docs[{i}].page_content:\\n{doc.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55f2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_DT\\Prog\\_MyCode\\ed_AI_2025\\PortfolioGenAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings  # Alternative:  from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    # Try:          from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# define how to map a string (can be a sentence or a paragraph) to a vector\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # This model is small (~80MB), fast on CPU, good for English # Alternative: ultra-fast memory-light (~45MB): model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\" # Alternative: embeddings = OpenAIEmbeddings()\n",
    "# # Test: one embed string\n",
    "# embed_test = embeddings.embed_query(\"What company is the report about?\")\n",
    "# print(f'Test: len(embeddings)={len(embed_test)}, embeddings[:5]={embed_test[:5]}')  # Should return a 384-dim vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e4c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of vectors in the DB is 16\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m db.save_local(\u001b[33m\"\u001b[39m\u001b[33mdata/faiss_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m embeddings = HuggingFaceEmbeddings(model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# This model is small (~80MB), fast on CPU, good for English # Alternative: ultra-fast memory-light (~45MB): model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\" # Alternative: embeddings = OpenAIEmbeddings()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m db = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_local\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/faiss_index\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\_DT\\Prog\\_MyCode\\ed_AI_2025\\PortfolioGenAI\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1190\u001b[39m, in \u001b[36mFAISS.load_local\u001b[39m\u001b[34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[39m\n\u001b[32m   1176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load FAISS index, docstore, and index_to_docstore_id from disk.\u001b[39;00m\n\u001b[32m   1177\u001b[39m \n\u001b[32m   1178\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1187\u001b[39m \u001b[33;03m        arbitrary code on your machine.\u001b[39;00m\n\u001b[32m   1188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_deserialization:\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe de-serialization relies loading a pickle file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPickle files can be modified to deliver a malicious payload that \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresults in execution of arbitrary code on your machine.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1194\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou will need to set `allow_dangerous_deserialization` to `True` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33menable deserialization. If you do this, make sure that you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1196\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrust the source of the data. For example, if you are loading a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile that you created, and know that no one else has modified the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile, then this is safe to do. Do not set this to `True` if you are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloading a file from an untrusted source (e.g., some random site on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe internet.).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1201\u001b[39m     )\n\u001b[32m   1202\u001b[39m path = Path(folder_path)\n\u001b[32m   1203\u001b[39m \u001b[38;5;66;03m# load index separately since it is not picklable\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.)."
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS  # Vector Database (indexes); alternatives: Pinecon, Weaviate\n",
    "db = FAISS.from_documents(docs, embeddings)  # create a DB of vector embeddings from the docs\n",
    "print(f'The number of vectors in the DB is {db.index.ntotal}')\n",
    "\n",
    "db.save_local(\"data/faiss_index\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # This model is small (~80MB), fast on CPU, good for English # Alternative: ultra-fast memory-light (~45MB): model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\" # Alternative: embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.load_local(\"data/faiss_index\", embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0bbd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectordb_from_report(report_filename: str) -> FAISS:\n",
    "    return db # TODO: Implement this function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a3262",
   "metadata": {},
   "source": [
    "# Simularity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "def prep_simularity_search_query(metrics, metric_id:int) -> str:\n",
    "\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"{metric_name} metric:\\n{metric_description}\"\"\",\n",
    "        input_variables=[\"metric_name\", \"metric_description\"]\n",
    "    )\n",
    "\n",
    "    query = user_prompt.format(\n",
    "        metric_name = metrics[metrics[\"MetricID\"]==metric_id][\"MetricName\"].values[0],\n",
    "        metric_description = metrics[metrics[\"MetricID\"]==metric_id][\"MetricDescription\"].values[0]).content\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f216d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simularity_search(query:str, db, chunks_number:int=4) -> str:\n",
    "    docs = db.similarity_search(query, k=chunks_number)  # find chunks_number docs similar to the user's query; FAISS does the similarity search\n",
    "    new_company_report_chunks_summary = \" \".join([doc.page_content for doc in docs])  # combine \"page_content\" fields from each of the found docs\n",
    "    return new_company_report_chunks_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce516c88",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3fecc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics_filename = 'data/metrics.csv'\n",
    "metrics = pd.read_csv(metrics_filename)\n",
    "# print(metrics.iloc[0][\"MetricDescription\"])\n",
    "# metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "78857215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples_filename = 'data/train_examples.csv'\n",
    "train_examples = pd.read_csv(train_examples_filename)\n",
    "# train_examples.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5fb9951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prompt_inputs(new_company, metrics, metric_id, train_examples, new_company_report_chunks_summary) -> dict:\n",
    "    prompt_inputs = {\n",
    "        'new_company': new_company,\n",
    "        'metric_name': metrics[metrics[\"MetricID\"]==metric_id][\"MetricName\"].values[0],\n",
    "        'metric_description': metrics[metrics[\"MetricID\"]==metric_id][\"MetricDescription\"].values[0]\n",
    "    }\n",
    "\n",
    "    # add inputs from exaples\n",
    "    df = train_examples[train_examples['MetricID']==metric_id].reset_index(drop=True)\n",
    "    assert len(df) == 3, \"Expected exactly 3 example companies for 1 metric\"  #Safety check  #FIXME:if we add more trainig examples later\n",
    "\n",
    "    prompt_inputs['Company_1'] = df.loc[0, 'Company']\n",
    "    prompt_inputs['Score_1'] = df.loc[0, 'Score']\n",
    "    prompt_inputs['Reasons1_1'] = df.loc[0, 'Reason1']\n",
    "    prompt_inputs['Reasons2_1'] = df.loc[0, 'Reason2']\n",
    "    prompt_inputs['Reasons3_1'] = df.loc[0, 'Reason3']\n",
    "    \n",
    "    prompt_inputs['Company_2'] = df.loc[1, 'Company']\n",
    "    prompt_inputs['Score_2'] = df.loc[1, 'Score']\n",
    "    prompt_inputs['Reasons1_2'] = df.loc[1, 'Reason1']\n",
    "    prompt_inputs['Reasons2_2'] = df.loc[1, 'Reason2']\n",
    "    prompt_inputs['Reasons3_2'] = df.loc[1, 'Reason3']\n",
    "    \n",
    "    prompt_inputs['Company_3'] = df.loc[2, 'Company']\n",
    "    prompt_inputs['Score_3'] = df.loc[2, 'Score']\n",
    "    prompt_inputs['Reasons1_3'] = df.loc[2, 'Reason1']\n",
    "    prompt_inputs['Reasons2_3'] = df.loc[2, 'Reason2']\n",
    "    prompt_inputs['Reasons3_3'] = df.loc[2, 'Reason3']\n",
    "\n",
    "    prompt_inputs['new_company_report_chunks_summary'] = new_company_report_chunks_summary\n",
    "    \n",
    "    return prompt_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2e7da845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template('You are a sustainability consultant tasked to score a company against the provided metric. Score can be: 1, 2, or 3.')\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"# You need to score the company \"{new_company}\" against the metric and criteria (provided below) and provide 3 reasons for the score.\n",
    "        ## The output should be a JSON object with the following fields (no other explanation or text or fields are allowed):\n",
    "        - Company: the name of the company\n",
    "        - Score: the score of the company\n",
    "        - Reasons1: first reason for the score\n",
    "        - Reasons2: second reason for the score\n",
    "        - Reasons3: third reason for the score\n",
    "    \n",
    "    Metric name: {metric_name}\n",
    "    Scoring criteria: {metric_description}\n",
    "\n",
    "    # Below are examples of the scoring applied to 3 companies:\n",
    "    Company 1: {Company_1}\n",
    "    Score: {Score_1}\n",
    "    Reason 1: {Reasons1_1}\n",
    "    Reason 2: {Reasons2_1}\n",
    "    Reason 3: {Reasons3_1}\n",
    "\n",
    "    Company 2: {Company_2}\n",
    "    Score: {Score_2}\n",
    "    Reason 1: {Reasons1_2}\n",
    "    Reason 2: {Reasons2_2}\n",
    "    Reason 3: {Reasons3_2}\n",
    "\n",
    "    Company 3: {Company_3}\n",
    "    Score: {Score_3}\n",
    "    Reason 1: {Reasons1_3}\n",
    "    Reason 2: {Reasons2_3}\n",
    "    Reason 3: {Reasons3_3}\n",
    "    \n",
    "    # The report of the company \"{new_company}\" is:\n",
    "    {new_company_report_chunks_summary}\n",
    "    \"\"\",\n",
    "\n",
    "    input_variables=[\"metric_name\", \"metric_description\", \"new_company\",\n",
    "        \"Company_1\", \"Score_1\", \"Reasons1_1\", \"Reasons2_1\", \"Reasons3_1\",\n",
    "        \"Company_2\", \"Score_2\", \"Reasons1_2\", \"Reasons2_2\", \"Reasons3_2\",\n",
    "        \"Company_3\", \"Score_3\", \"Reasons1_3\", \"Reasons2_3\", \"Reasons3_3\", \"new_company_report_chunks_summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ba951",
   "metadata": {},
   "source": [
    "Now we can merge the system and user prompts into a full chat prompt using the `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2d78faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt template 1: create an article title\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad6879",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2693c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d8cbe",
   "metadata": {},
   "source": [
    "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a9c27e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt.format(**prompt_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ac1e",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_company = \"IBM\"  # FIXME\n",
    "# create new_company_scores_df\n",
    "\n",
    "for i in range(1):  #FIXME: range(len(metrics)):\n",
    "    metric_id = metrics.iloc[i][\"MetricID\"]\n",
    "    print(f\"MetricID: {metric_id}\")\n",
    "\n",
    "    query = prep_simularity_search_query(metrics, metric_id)\n",
    "    print(query)\n",
    "\n",
    "    new_company_report_chunks_summary = simularity_search(query, db, chunks_number=3)\n",
    "    print(f\"new_company_report_chunks_summary: {new_company_report_chunks_summary}\")\n",
    "\n",
    "    prompt_inputs = prep_prompt_inputs(new_company, metrics, metric_id, train_examples, new_company_report_chunks_summary)\n",
    "    output = llm.invoke(prompt.format(**prompt_inputs))  # run the LLM to score the new company and extract 3 reasons for the score\n",
    "    \n",
    "    # append res_i to new_company_scores_df\n",
    "    # save new_company_scores_df to new_company_scores.csv\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0c945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n  \"Company\": \"IBM\",\\n  \"Score\": 3,\\n  \"Reasons1\": \"IBM has committed to net zero GHG emissions by 2030, encompassing Scope 1 emissions.\",\\n  \"Reasons2\": \"IBM provides a detailed roadmap with specific targets and initiatives for reducing Scope 1 emissions year by year.\",\\n  \"Reasons3\": \"IBM has already implemented several initiatives and achieved significant reductions in its Scope 1 emissions, demonstrating progress towards its 2030 goal.\"\\n}\\n```', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--9f10a023-373d-436b-bc47-c9bc44f405a4-0', usage_metadata={'input_tokens': 656, 'output_tokens': 114, 'total_tokens': 770, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(prompt.format(**prompt_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9e39e615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Company': 'IBM',\n",
       " 'Score': 3,\n",
       " 'Reasons1': 'IBM has committed to net zero GHG emissions by 2030, encompassing Scope 1 emissions.',\n",
       " 'Reasons2': 'IBM provides a detailed roadmap with specific targets and initiatives for reducing Scope 1 emissions year by year.',\n",
       " 'Reasons3': 'IBM has already implemented several initiatives and achieved significant reductions in its Scope 1 emissions, demonstrating progress towards its 2030 goal.'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Step 1: Extract the JSON part (removes ```json ... ```)\n",
    "raw_output = output.content\n",
    "json_str = re.sub(r\"^```json|```$\", \"\", raw_output.strip()).strip()\n",
    "\n",
    "# Step 2: Convert to Python dict\n",
    "parsed_output = json.loads(json_str)\n",
    "\n",
    "# Check the result\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3fc410e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>MetricID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Reasons1</th>\n",
       "      <th>Reasons2</th>\n",
       "      <th>Reasons3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>IBM has committed to net zero GHG emissions by...</td>\n",
       "      <td>IBM provides a detailed roadmap with specific ...</td>\n",
       "      <td>IBM has already implemented several initiative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company  MetricID  Score                                           Reasons1  \\\n",
       "0     IBM         1      3  IBM has committed to net zero GHG emissions by...   \n",
       "\n",
       "                                            Reasons2  \\\n",
       "0  IBM provides a detailed roadmap with specific ...   \n",
       "\n",
       "                                            Reasons3  \n",
       "0  IBM has already implemented several initiative...  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_i = pd.DataFrame(parsed_output, index=[0])\n",
    "res_i.insert(1, \"MetricID\", metric_id)\n",
    "res_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2936b",
   "metadata": {},
   "source": [
    "Now chain the `prompt` template and the `llm` object defined earlier to create an LLM chain for **prompt formatting > llm generation > get output**.\n",
    "\n",
    "Let's use __LCEL__ to construct the chain: define inputs with `{\"article\": lambda x: x[\"article\"]}` and use the pipe operator (`|`) to feed the output from its left into the input to its right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73c7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d434bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain1: inputs:article / output:article_title\n",
    "chain = (\n",
    "    # {\n",
    "    #     \"article\": lambda x: x[\"article\"],\n",
    "    #     \"article_title\": lambda x: x[\"article_title\"]\n",
    "    # }\n",
    "    # | \n",
    "    prompt\n",
    "    | llm\n",
    "    | {\"response\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7615599",
   "metadata": {},
   "source": [
    "This chain creates scores the new company and procides 3 reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "75a772e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"Company\": \"IBM\",\\n  \"Score\": 3,\\n  \"Reasons1\": \"IBM has committed to net zero GHG emissions by 2030, encompassing Scope 1 emissions.\",\\n  \"Reasons2\": \"IBM provides a detailed roadmap with specific initiatives and timelines for achieving its net zero target, including interim goals and progress reporting.\",\\n  \"Reasons3\": \"IBM has already implemented several initiatives and demonstrated progress towards its GHG reduction targets, indicating that the first steps have been taken.\"\\n}\\n```'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = chain.invoke({\n",
    "#     \"article\": article,\n",
    "#     \"article_title\": article_title_msg[\"article_title\"]\n",
    "# })\n",
    "res = chain.invoke(prompt_inputs)\n",
    "res[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad28f18",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> __NEXT__ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5611b3",
   "metadata": {},
   "source": [
    "Since we output _several fields_ we'll specify for the LLM to use __structured outputs__ to make the generated fields aligned with our requirements.\n",
    "or this we create a _pydantic object_ and describe the required output format - this format description is then passed to our model using the `with_structured_output` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f0a4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Metric_Score(BaseModel):\n",
    "    paragraph_original: str = Field(description=\"The original paragraph\")\n",
    "    paragraph_edited: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(description=\"Feedback on the original paragraph\")\n",
    "\n",
    "llm_structured = llm.with_structured_output(Metric_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c24f9d",
   "metadata": {},
   "source": [
    "Let's combine all this into chain3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af74e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain3: inputs:article / output:paragraph\n",
    "chain3 = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt\n",
    "    | llm_structured\n",
    "    | {\n",
    "        \"paragraph_original\": lambda x: x.paragraph_original,\n",
    "        \"paragraph_edited\": lambda x: x.paragraph_edited,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3273870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph_original': 'I tackled a pressing issue for a large life insurance client: their computational model for projecting financial outcomes and predicting balance sheets over multiple quarters for life insurance products was prohibitively slow and costly. The client’s leadership needed a capability to run ad-hoc what-if scenarios, but the model’s runtime prevented near-real-time risk analytics.',\n",
       " 'paragraph_edited': 'I tackled a pressing issue for a large life insurance client: their computational model for projecting financial outcomes and predicting balance sheets over multiple quarters for life insurance products was prohibitively slow and costly. This inefficiency hindered their ability to respond quickly to market changes and increased the risk of inaccurate financial forecasting. The client’s leadership needed a capability to run ad-hoc what-if scenarios, but the model’s runtime prevented near-real-time risk analytics.',\n",
       " 'feedback': \"The original paragraph is well-written and clearly explains the problem. However, it could be improved by adding a sentence or two that highlight the impact of the slow and costly model on the client's business operations. This would provide additional context and emphasize the importance of solving the problem.\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_and_feedback = chain.invoke({\"article\": article})\n",
    "paragraph_and_feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
