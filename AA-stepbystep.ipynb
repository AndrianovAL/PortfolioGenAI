{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c1b596",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4cb8e",
   "metadata": {},
   "source": [
    "In this example, I am using LangChain to build a simple LLM-powered assistant. I will focus on using:\n",
    "1. Google Gemini\n",
    "2. OpenAI gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdf314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   langchain-core==0.3.33 \\\n",
    "#   langchain-openai==0.3.3 \\\n",
    "#   langchain-community==0.3.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576be773",
   "metadata": {},
   "source": [
    "First need to initialize an LLM. I'll use _Google Gemini_ or _OpenAI 'gpt-4.1-nano'_ model. You can get an API key from [OpenAI](https://platform.openai.com/settings/organization/api-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec980a",
   "metadata": {},
   "source": [
    "# Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48382bba",
   "metadata": {},
   "source": [
    "I'll take a `new_report.txt`, `metrics.csv`, and `training_examples.csv` and use LangChain to:\n",
    "1. Score the new report\n",
    "2. Extract from the new_report 3 pieces of supporting evidence for this Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ea039",
   "metadata": {},
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_Provider = \"GOOGLE\"\n",
    "# LLM_Provider = \"OPENAI\"\n",
    "\n",
    "if AI_Provider == \"GOOGLE\":\n",
    "    llm_model = \"gemini-2.0-flash\"\n",
    "elif AI_Provider == \"OPENAI\":\n",
    "    llm_model = \"gpt-4.1-nano\"  # less expensive than gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3cccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GOOGLE_API_KEY loaded successfully (not printing it for security).\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "load_dotenv()  # Try to load local .env file (for local dev); silently skip if not found (for CI)\n",
    "os.environ[f\"{AI_Provider}_API_KEY\"] = os.getenv(f\"{AI_Provider}_API_KEY\") or getpass(f\"Enter {AI_Provider} API Key: \")  # Get API key from environment or user input\n",
    "if os.getenv(f\"{AI_Provider}_API_KEY\") is None:\n",
    "    raise ValueError(f\"❌ {AI_Provider}_API_KEY not found. Make sure it's in your .env file or set as a GitHub Action secret.\")\n",
    "else:\n",
    "    print(f\"✅ {AI_Provider}_API_KEY loaded successfully (not printing it for security).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My LLM version is: I'm currently running on the Gemini Pro model.\n"
     ]
    }
   ],
   "source": [
    "if AI_Provider == \"GOOGLE\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI as ChatLLM\n",
    "elif AI_Provider == \"OPENAI\":\n",
    "    from langchain_openai import ChatOpenAI as ChatLLM\n",
    "\n",
    "llm = ChatLLM(temperature=0.0, model=llm_model)  # For normal accurate responses\n",
    "# print(\"My LLM version is:\", llm.invoke(\"What LLM version are you?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5b59f",
   "metadata": {},
   "source": [
    "Since we output _several fields_ we'll specify for the LLM to use __structured outputs__ to make the generated fields aligned with our requirements.\n",
    "or this we create a _pydantic object_ and describe the required output format - this format description is then passed to our model using the `with_structured_output` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CompanyScore(BaseModel):\n",
    "    Company: str = Field(..., description=\"The name of the company\")\n",
    "    MetricID: int = Field(..., description=\"The ID of the metric\")\n",
    "    Score: int = Field(..., description=\"Score must be 1, 2, or 3\")  # Accepts only int values 1, 2, or 3\n",
    "    Reason1: str = Field(..., description=\"First reason for the score\")\n",
    "    Reason2: str = Field(..., description=\"Second reason for the score\")\n",
    "    Reason3: str = Field(..., description=\"Third reason for the score\")\n",
    "\n",
    "\n",
    "llm_structured = llm.with_structured_output(CompanyScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6108c9b",
   "metadata": {},
   "source": [
    "# Create VectorDB from TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5cc5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250  # FIXME: calibrate\n",
    "chunk_overlap = 50  # FIXME: calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92edff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report loaded successfully!\n",
      "Report length: 2591 characters\n",
      "First 200 characters of the report:\n",
      "Balanced Power of IBM\n",
      "\n",
      "IBM’s latest sustainability report opens with a bold statement: “IBM is committed to achieving net-zero GHG Scope 1 emissions by 2050 through a comprehensive, three-phase decarb...\n"
     ]
    }
   ],
   "source": [
    "# Read the IBM report file into a string\n",
    "with open('data/new_report_IBM.txt', 'r', encoding='utf-8') as file:\n",
    "    report = file.read()\n",
    "print(\"Report loaded successfully!\")\n",
    "print(f\"Report length: {len(report)} characters\")\n",
    "print(\"First 200 characters of the report:\")\n",
    "print(report[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faebc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chunks is 16\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # to split YouTube transcript into chunks\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=report)]  # wrap the report string in a Document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)  # initilize the text splitter\n",
    "docs = text_splitter.split_documents(documents)  # split report into overlapping chunks\n",
    "print(f'The number of chunks is {len(docs)}')\n",
    "# # test\n",
    "# print(f'Test: docs[0].page_content:\\n{docs[0].page_content}')\n",
    "# for i, doc in enumerate(docs):\n",
    "#     print(f'Test: docs[{i}].page_content:\\n{doc.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55f2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_DT\\Prog\\_MyCode\\ed_AI_2025\\PortfolioGenAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings  # Alternative:  from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    # Try:          from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# define how to map a string (can be a sentence or a paragraph) to a vector\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # This model is small (~80MB), fast on CPU, good for English # Alternative: ultra-fast memory-light (~45MB): model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\" # Alternative: embeddings = OpenAIEmbeddings()\n",
    "# # Test: one embed string\n",
    "# embed_test = embeddings.embed_query(\"What company is the report about?\")\n",
    "# print(f'Test: len(embeddings)={len(embed_test)}, embeddings[:5]={embed_test[:5]}')  # Should return a 384-dim vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e4c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of vectors in the DB is 16\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS  # Vector Database (indexes); alternatives: Pinecon, Weaviate\n",
    "db = FAISS.from_documents(docs, embeddings)  # create a DB of vector embeddings from the docs\n",
    "print(f'The number of vectors in the DB is {db.index.ntotal}')\n",
    "\n",
    "# db.save_local(\"data/faiss_index\")\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # This model is small (~80MB), fast on CPU, good for English # Alternative: ultra-fast memory-light (~45MB): model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\" # Alternative: embeddings = OpenAIEmbeddings()\n",
    "# db = FAISS.load_local(\"data/faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0bbd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectordb_from_report(report_filename: str) -> FAISS:\n",
    "    return db # TODO: Implement this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a3262",
   "metadata": {},
   "source": [
    "# Simularity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf26e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "def prep_simularity_search_query(metrics, metric_id:int) -> str:\n",
    "\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"{metric_name} metric:\\n{metric_description}\"\"\",\n",
    "        input_variables=[\"metric_name\", \"metric_description\"]\n",
    "    )\n",
    "\n",
    "    query = user_prompt.format(\n",
    "        metric_name = metrics[metrics[\"MetricID\"]==metric_id][\"MetricName\"].values[0],\n",
    "        metric_description = metrics[metrics[\"MetricID\"]==metric_id][\"MetricDescription\"].values[0]).content\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f216d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simularity_search(query:str, db, chunks_number:int=4) -> str:\n",
    "    docs = db.similarity_search(query, k=chunks_number)  # find chunks_number docs similar to the user's query; FAISS does the similarity search\n",
    "    new_company_report_chunks_summary = \" \".join([doc.page_content for doc in docs])  # combine \"page_content\" fields from each of the found docs\n",
    "    return new_company_report_chunks_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce516c88",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fecc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics_filename = 'data/metrics.csv'\n",
    "metrics = pd.read_csv(metrics_filename)\n",
    "# print(metrics.iloc[0][\"MetricDescription\"])\n",
    "# metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78857215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples_filename = 'data/train_examples.csv'\n",
    "train_examples = pd.read_csv(train_examples_filename)\n",
    "# train_examples.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb9951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prompt_inputs(new_company, metrics, metric_id, train_examples, new_company_report_chunks_summary) -> dict:\n",
    "    prompt_inputs = {\n",
    "        'new_company': new_company,\n",
    "        'metric_id': metric_id,\n",
    "        'metric_name': metrics[metrics[\"MetricID\"]==metric_id][\"MetricName\"].values[0],\n",
    "        'metric_description': metrics[metrics[\"MetricID\"]==metric_id][\"MetricDescription\"].values[0]\n",
    "    }\n",
    "\n",
    "    # add inputs from exaples\n",
    "    df = train_examples[train_examples['MetricID']==metric_id].reset_index(drop=True)\n",
    "    assert len(df) == 3, \"Expected exactly 3 example companies for 1 metric\"  #Safety check  #FIXME:if we add more trainig examples later\n",
    "\n",
    "    prompt_inputs['Company_1'] = df.loc[0, 'Company']\n",
    "    prompt_inputs['Score_1'] = df.loc[0, 'Score']\n",
    "    prompt_inputs['Reason1_1'] = df.loc[0, 'Reason1']\n",
    "    prompt_inputs['Reason2_1'] = df.loc[0, 'Reason2']\n",
    "    prompt_inputs['Reason3_1'] = df.loc[0, 'Reason3']\n",
    "    \n",
    "    prompt_inputs['Company_2'] = df.loc[1, 'Company']\n",
    "    prompt_inputs['Score_2'] = df.loc[1, 'Score']\n",
    "    prompt_inputs['Reason1_2'] = df.loc[1, 'Reason1']\n",
    "    prompt_inputs['Reason2_2'] = df.loc[1, 'Reason2']\n",
    "    prompt_inputs['Reason3_2'] = df.loc[1, 'Reason3']\n",
    "    \n",
    "    prompt_inputs['Company_3'] = df.loc[2, 'Company']\n",
    "    prompt_inputs['Score_3'] = df.loc[2, 'Score']\n",
    "    prompt_inputs['Reason1_3'] = df.loc[2, 'Reason1']\n",
    "    prompt_inputs['Reason2_3'] = df.loc[2, 'Reason2']\n",
    "    prompt_inputs['Reason3_3'] = df.loc[2, 'Reason3']\n",
    "\n",
    "    prompt_inputs['new_company_report_chunks_summary'] = new_company_report_chunks_summary\n",
    "    \n",
    "    return prompt_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e7da845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template('You are a sustainability consultant tasked to score a company against the provided metric. Score can be: 1, 2, or 3.')\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"# You need to score the company \"{new_company}\" against the metric and criteria (provided below) and provide 3 reasons for the score.\n",
    "        ## The output should be a JSON object with the following fields (no other explanation or text or fields are allowed):\n",
    "        - Company: the name of the company\n",
    "        - MetricID: MetricID\n",
    "        - Score: the score of the company\n",
    "        - Reason1: first reason for the score\n",
    "        - Reason2: second reason for the score\n",
    "        - Reason3: third reason for the score\n",
    "    \n",
    "    MetricID: {metric_id}\n",
    "    Metric name: {metric_name}\n",
    "    Scoring criteria: {metric_description}\n",
    "\n",
    "    # Below are examples of the scoring applied to 3 companies:\n",
    "    Company 1: {Company_1}\n",
    "    Score: {Score_1}\n",
    "    Reason 1: {Reason1_1}\n",
    "    Reason 2: {Reason2_1}\n",
    "    Reason 3: {Reason3_1}\n",
    "\n",
    "    Company 2: {Company_2}\n",
    "    Score: {Score_2}\n",
    "    Reason 1: {Reason1_2}\n",
    "    Reason 2: {Reason2_2}\n",
    "    Reason 3: {Reason3_2}\n",
    "\n",
    "    Company 3: {Company_3}\n",
    "    Score: {Score_3}\n",
    "    Reason 1: {Reason1_3}\n",
    "    Reason 2: {Reason2_3}\n",
    "    Reason 3: {Reason3_3}\n",
    "    \n",
    "    # The report of the company \"{new_company}\" is:\n",
    "    {new_company_report_chunks_summary}\n",
    "    \"\"\",\n",
    "\n",
    "    input_variables=[\"metric_id\", \"metric_name\", \"metric_description\", \"new_company\",\n",
    "        \"Company_1\", \"Score_1\", \"Reason1_1\", \"Reason2_1\", \"Reason3_1\",\n",
    "        \"Company_2\", \"Score_2\", \"Reason1_2\", \"Reason2_2\", \"Reason3_2\",\n",
    "        \"Company_3\", \"Score_3\", \"Reason1_3\", \"Reason2_3\", \"Reason3_3\", \"new_company_report_chunks_summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ba951",
   "metadata": {},
   "source": [
    "Now we can merge the system and user prompts into a full chat prompt using the `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d78faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt template 1: create an article title\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad6879",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2693c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d8cbe",
   "metadata": {},
   "source": [
    "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c27e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt.format(**prompt_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ac1e",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b994818",
   "metadata": {},
   "source": [
    "Now chain the `prompt` template and the `llm` object defined earlier to create an LLM chain for **prompt formatting > llm generation > get output**.\n",
    "\n",
    "Let's use __LCEL__ to construct the chain: define inputs with `{\"metric_id\": lambda x: x[\"metric_id\"], ...}` and use the pipe operator (`|`) to feed the output from its left into the input to its right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e043d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain outputs (for the given metric) the new company's score and provides 3 reasons\n",
    "chain = (\n",
    "    {\n",
    "        \"metric_id\": lambda x: x[\"metric_id\"],\n",
    "        \"metric_name\": lambda x: x[\"metric_name\"],\n",
    "        \"metric_description\": lambda x: x[\"metric_description\"],\n",
    "        \"new_company\": lambda x: x[\"new_company\"],\n",
    "        \"Company_1\": lambda x: x[\"Company_1\"],\n",
    "        \"Score_1\": lambda x: x[\"Score_1\"],\n",
    "        \"Reason1_1\": lambda x: x[\"Reason1_1\"],\n",
    "        \"Reason2_1\": lambda x: x[\"Reason2_1\"],\n",
    "        \"Reason3_1\": lambda x: x[\"Reason3_1\"],\n",
    "        \"Company_2\": lambda x: x[\"Company_2\"],\n",
    "        \"Score_2\": lambda x: x[\"Score_2\"],\n",
    "        \"Reason1_2\": lambda x: x[\"Reason1_2\"],\n",
    "        \"Reason2_2\": lambda x: x[\"Reason2_2\"],\n",
    "        \"Reason3_2\": lambda x: x[\"Reason3_2\"],\n",
    "        \"Company_3\": lambda x: x[\"Company_3\"],\n",
    "        \"Score_3\": lambda x: x[\"Score_3\"],\n",
    "        \"Reason1_3\": lambda x: x[\"Reason1_3\"],\n",
    "        \"Reason2_3\": lambda x: x[\"Reason2_3\"],\n",
    "        \"Reason3_3\": lambda x: x[\"Reason3_3\"],\n",
    "        \"new_company_report_chunks_summary\": lambda x: x[\"new_company_report_chunks_summary\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_structured\n",
    "    | {\n",
    "        \"Company\": lambda llm_output: llm_output.Company,\n",
    "        \"MetricID\": lambda llm_output: llm_output.MetricID,\n",
    "        \"Score\": lambda llm_output: llm_output.Score,\n",
    "        \"Reason1\": lambda llm_output: llm_output.Reason1,\n",
    "        \"Reason2\": lambda llm_output: llm_output.Reason2,\n",
    "        \"Reason3\": lambda llm_output: llm_output.Reason3\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24afb0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>MetricID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Reason1</th>\n",
       "      <th>Reason2</th>\n",
       "      <th>Reason3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Company, MetricID, Score, Reason1, Reason2, Reason3]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_company = \"IBM\"  # FIXME\n",
    "\n",
    "new_company_scores_df = pd.DataFrame(columns=train_examples.columns)\n",
    "new_company_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3eb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run 1 iteration of the below for loop\n",
    "# i = 0\n",
    "\n",
    "# metric_id = metrics.iloc[i][\"MetricID\"]\n",
    "# print(f\"> MetricID: {metric_id}\")\n",
    "\n",
    "# query = prep_simularity_search_query(metrics, metric_id)\n",
    "# print(query)\n",
    "\n",
    "# new_company_report_chunks_summary = simularity_search(query, db, chunks_number=3)\n",
    "# print(f\"\\n> new_company_report_chunks_summary: {new_company_report_chunks_summary}\")\n",
    "\n",
    "# prompt_inputs = prep_prompt_inputs(new_company, metrics, metric_id, train_examples, new_company_report_chunks_summary)\n",
    "\n",
    "# # output = llm_structured.invoke(prompt.format(**prompt_inputs))  # run the LLM to score the new company and extract 3 reasons for the score\n",
    "# output = chain.invoke(prompt_inputs)\n",
    "# print(\"\\n> \", type(output))\n",
    "# print(\"\\n> \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dadb9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>MetricID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Reason1</th>\n",
       "      <th>Reason2</th>\n",
       "      <th>Reason3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>IBM will get to net zero by 2050 via a three-p...</td>\n",
       "      <td>It'll allow GHG S1 to reach these levels: 2020...</td>\n",
       "      <td>IBM hit these GHG S1 levels: 2020 - 80 tons, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBM</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>IBM employees run 100m in 14.8s.</td>\n",
       "      <td>IBM employees can do 40 pushups.</td>\n",
       "      <td>IBM employees can do 60 jumps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Panelist said math is just a rough guideline.</td>\n",
       "      <td>Modern discourse sometimes sacrifices intellec...</td>\n",
       "      <td>Some educational narratives veer into pseudosc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company MetricID Score                                            Reason1  \\\n",
       "0     IBM        1     2  IBM will get to net zero by 2050 via a three-p...   \n",
       "1     IBM        2     3                   IBM employees run 100m in 14.8s.   \n",
       "2     IBM        3     1      Panelist said math is just a rough guideline.   \n",
       "\n",
       "                                             Reason2  \\\n",
       "0  It'll allow GHG S1 to reach these levels: 2020...   \n",
       "1                   IBM employees can do 40 pushups.   \n",
       "2  Modern discourse sometimes sacrifices intellec...   \n",
       "\n",
       "                                             Reason3  \n",
       "0  IBM hit these GHG S1 levels: 2020 - 80 tons, 2...  \n",
       "1                     IBM employees can do 60 jumps.  \n",
       "2  Some educational narratives veer into pseudosc...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(metrics)):\n",
    "    metric_id = metrics.iloc[i][\"MetricID\"]\n",
    "    # print(f\"MetricID: {metric_id}\")\n",
    "\n",
    "    query = prep_simularity_search_query(metrics, metric_id)\n",
    "    # print(query)\n",
    "\n",
    "    new_company_report_chunks_summary = simularity_search(query, db, chunks_number=3)\n",
    "    # print(f\"new_company_report_chunks_summary: {new_company_report_chunks_summary}\")\n",
    "\n",
    "    prompt_inputs = prep_prompt_inputs(new_company, metrics, metric_id, train_examples, new_company_report_chunks_summary)\n",
    "    output = llm.invoke(prompt.format(**prompt_inputs))  # run the LLM to score the new company and extract 3 reasons for the score\n",
    "    \n",
    "    # llm_output = llm_structured.invoke(prompt.format(**prompt_inputs))\n",
    "    # print(output)\n",
    "    output = chain.invoke(prompt_inputs)\n",
    "    new_company_scores_df = pd.concat([new_company_scores_df, pd.DataFrame(output, index=[0])], ignore_index=True)\n",
    "\n",
    "# save new_company_scores_df to new_company_scores.csv\n",
    "new_company_scores_df.to_csv(\"data/new_company_scores.csv\", index=False)\n",
    "new_company_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad28f18",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> __NEXT__ </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
